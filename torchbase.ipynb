{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 35756.28986138456\n",
      "100 177.148647601135\n",
      "200 120.58997865764785\n",
      "300 83.05569414732712\n",
      "400 58.140035568255946\n",
      "500 41.59654246412779\n",
      "600 30.6090279163518\n",
      "700 23.30947508737842\n",
      "800 18.458544147598218\n",
      "900 15.233810604233287\n",
      "1000 13.08938628726138\n",
      "1100 11.66284491381254\n",
      "1200 10.713500547732561\n",
      "1300 10.081469636576475\n",
      "1400 9.660512615280627\n",
      "1500 9.380013024888907\n",
      "1600 9.193017053691861\n",
      "1700 9.06829344049693\n",
      "1800 8.985060976481291\n",
      "1900 8.929486420314944\n",
      "Result: y = 0.003971434192688835 + 0.8643589833540068x + -0.0006851384831890389 x^2 + -0.09441400315611127 x^3\n",
      "6.872527442988272\n"
     ]
    }
   ],
   "source": [
    "#무작위 입력과 출력 데이터\n",
    "x = np.linspace(-math.pi, math.pi, 2000)\n",
    "y = np.sin(x)\n",
    "\n",
    "#무작위 가중치 초기화\n",
    "a = np.random.randn()\n",
    "b = np.random.randn()\n",
    "c = np.random.randn()\n",
    "d = np.random.randn()\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(2000):\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "    loss = np.square(y_pred - y).sum() # y_pred - y를 array 단위로 반환\n",
    "    if t % 100 == 0:\n",
    "        print(t,loss)\n",
    "        \n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "    \n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "    \n",
    "    \n",
    "print(f'Result: y = {a} + {b}x + {c} x^2 + {d} x^3')\n",
    "print(grad_y_pred.sum())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 69729.0703125\n",
      "199 69729.0703125\n",
      "299 69729.0703125\n",
      "399 69729.0703125\n",
      "499 69729.0703125\n",
      "599 69729.0703125\n",
      "699 69729.0703125\n",
      "799 69729.0703125\n",
      "899 69729.0703125\n",
      "999 69729.0703125\n",
      "1099 69729.0703125\n",
      "1199 69729.0703125\n",
      "1299 69729.0703125\n",
      "1399 69729.0703125\n",
      "1499 69729.0703125\n",
      "1599 69729.0703125\n",
      "1699 69729.0703125\n",
      "1799 69729.0703125\n",
      "1899 69729.0703125\n",
      "1999 69729.0703125\n",
      "Result: y = 0.050961412489414215 + 0.5636866092681885 x + -0.457431823015213 x^2 + 0.4216707646846771 x^3\n",
      "tensor(-0.0371)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "a = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.randn((), device=device, dtype=dtype,  requires_grad=True)\n",
    "c = torch.randn((), device=device, dtype =dtype,  requires_grad=True)\n",
    "d = torch.randn((), device=device, dtype= dtype,  requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "    \n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "        \n",
    "    loss.backward()\n",
    "    \n",
    "    \n",
    "    # a -= learning_rate * grad_a\n",
    "    # b -= learning_rate * grad_b\n",
    "    # c -= learning_rate * grad_c\n",
    "    # d -= learning_rate * grad_d\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "\n",
    "        # 가중치 갱신 후에는 변화도를 직접 0으로 만듭니다.\n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "\n",
    "print(f\"Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3\")\n",
    "print(grad_y_pred.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 2304.984619140625\n",
      "199 1559.2674560546875\n",
      "299 1056.774658203125\n",
      "399 717.8587036132812\n",
      "499 489.04974365234375\n",
      "599 334.4232177734375\n",
      "699 229.82244873046875\n",
      "799 158.98924255371094\n",
      "899 110.97191619873047\n",
      "999 78.38631439208984\n",
      "1099 56.248687744140625\n",
      "1199 41.19247817993164\n",
      "1299 30.940959930419922\n",
      "1399 23.95292854309082\n",
      "1499 19.18402862548828\n",
      "1599 15.925860404968262\n",
      "1699 13.697237968444824\n",
      "1799 12.171116828918457\n",
      "1899 11.124826431274414\n",
      "1999 10.406696319580078\n",
      "Result: y = -0.03296248987317085 + 0.83254075050354 x + 0.0056865764781832695 x^2 + -0.08988813310861588 x^3\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # GPU에서 실행하려면 이 주석을 제거하세요\n",
    "\n",
    "# 무작위로 입력과 출력 데이터를 생성합니다\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# 무작위로 가중치를 초기화합니다\n",
    "a = torch.randn((), device=device, dtype=dtype)\n",
    "b = torch.randn((), device=device, dtype=dtype)\n",
    "c = torch.randn((), device=device, dtype=dtype)\n",
    "d = torch.randn((), device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # 순전파 단계: 예측값 y를 계산합니다\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # 손실(loss)을 계산하고 출력합니다\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # 손실에 따른 a, b, c, d의 변화도(gradient)를 계산하고 역전파합니다.\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    # 가중치를 갱신합니다.\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 2733.918701171875\n",
      "199 1849.902099609375\n",
      "299 1253.91455078125\n",
      "399 851.7259521484375\n",
      "499 580.0520629882812\n",
      "599 396.3553771972656\n",
      "699 272.01824951171875\n",
      "799 187.77099609375\n",
      "899 130.6265106201172\n",
      "999 91.82345581054688\n",
      "1099 65.44573974609375\n",
      "1199 47.49459457397461\n",
      "1299 35.264305114746094\n",
      "1399 26.9222469329834\n",
      "1499 21.225696563720703\n",
      "1599 17.331174850463867\n",
      "1699 14.66565990447998\n",
      "1799 12.839181900024414\n",
      "1899 11.58619213104248\n",
      "1999 10.725648880004883\n",
      "Result: y = -0.036381132900714874 + 0.8305281400680542 x + 0.006276351865381002 x^2 + -0.08960186690092087 x^3\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import math\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # GPU에서 실행하려면 이 주석을 제거하세요\n",
    "\n",
    "# 입력값과 출력값을 갖는 텐서들을 생성합니다.\n",
    "# requires_grad=False가 기본값으로 설정되어 역전파 단계 중에 이 텐서들에 대한 변화도를\n",
    "# 계산할 필요가 없음을 나타냅니다.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# 가중치를 갖는 임의의 텐서를 생성합니다. 3차 다항식이므로 4개의 가중치가 필요합니다:\n",
    "# y = a + b x + c x^2 + d x^3\n",
    "# requires_grad=True로 설정하여 역전파 단계 중에 이 텐서들에 대한 변화도를 계산할 필요가\n",
    "# 있음을 나타냅니다.\n",
    "a = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # 순전파 단계: 텐서들 간의 연산을 사용하여 예측값 y를 계산합니다.\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # 텐서들간의 연산을 사용하여 손실(loss)을 계산하고 출력합니다.\n",
    "    # 이 때 손실은 (1,) shape을 갖는 텐서입니다.\n",
    "    # loss.item() 으로 손실이 갖고 있는 스칼라 값을 가져올 수 있습니다.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # autograd 를 사용하여 역전파 단계를 계산합니다. 이는 requires_grad=True를 갖는\n",
    "    # 모든 텐서들에 대한 손실의 변화도를 계산합니다.\n",
    "    # 이후 a.grad와 b.grad, c.grad, d.grad는 각각 a, b, c, d에 대한 손실의 변화도를\n",
    "    # 갖는 텐서가 됩니다.\n",
    "    loss.backward()\n",
    "\n",
    "    # 경사하강법(gradient descent)을 사용하여 가중치를 직접 갱신합니다.\n",
    "    # torch.no_grad()로 감싸는 이유는, 가중치들이 requires_grad=True 지만\n",
    "    # autograd에서는 이를 추적하지 않을 것이기 때문입니다.\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "\n",
    "        # 가중치 갱신 후에는 변화도를 직접 0으로 만듭니다.\n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "six",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
